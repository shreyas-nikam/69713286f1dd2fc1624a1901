id: 69713286f1dd2fc1624a1901_user_guide
summary: Lab 8: LLM Evaluation Harness (Inference & Fine-Tuning Risk) User Guide
feedback link: https://docs.google.com/forms/d/e/1FAIpQLSfWkOK-in_bMMoHSZfcIvAeO58PAH9wrDqcxnJABHaxiDqhSA/viewform?usp=sf_link
environments: Web
status: Published
# QuLab: LLM Evaluation Harness for Inference & Fine-Tuning Risk

## 1. Introduction and Data Loading
Duration: 05:00

Welcome to the QuLab LLM Evaluation Harness! This codelab will guide you through using a powerful Streamlit application designed to assess the trustworthiness and reliability of Large Language Models (LLMs), particularly in a Retrieval Augmented Generation (RAG) context. As **Alex**, an AI Risk Auditor at InnovateCorp, you're tasked with ensuring that our internal knowledge assistant, powered by LLMs, is robust, accurate, and safe before deployment. This involves not only evaluating a baseline model but also detecting potential risks or regressions introduced by fine-tuning.

This application is crucial for:
*   **Identifying Hallucinations**: Detecting when an LLM generates information that is not supported by its source material.
*   **Ensuring Faithfulness**: Verifying that the LLM's responses are grounded in provided or allowed sources.
*   **Managing Refusals**: Understanding how the LLM handles queries it cannot or should not answer, and preventing overly cautious or inappropriate refusals.
*   **Assessing Fine-Tuning Risks**: Comparing a fine-tuned model against a baseline to ensure improvements without introducing new undesirable behaviors (regressions).
*   **Generating Audit-Ready Artifacts**: Providing comprehensive documentation and evidence for compliance and governance.

The journey begins by providing the application with the necessary data: a set of prompts to test the LLM, and the corresponding outputs generated by your models.

### Loading Your Data

The evaluation harness requires the following data, typically in CSV format:

*   **Prompt Set:** Contains `prompt_id`, `prompt_text`, an optional `expected_answer` for direct comparison, and `allowed_sources` which specifies the relevant documents for RAG contexts.
*   **Model Outputs:** For each model (baseline and optionally fine-tuned), you'll provide a file containing `prompt_id`, the `llm_output` (the generated response), and `sources_cited` (any sources explicitly mentioned by the LLM).

This structure ensures that for each prompt, we have the original query, the desired outcome (if applicable), the context (allowed sources), and the actual LLM responses from different models.

You have two options to load data:

1.  **Load Sample Data:** For a quick start, click the "Load Sample Data" button. This will generate and load a predefined set of prompts and model outputs, including both baseline and fine-tuned models, allowing you to immediately explore the application's features.

    <aside class="positive">
    <b>Tip:</b> If you're new to the application, loading sample data is the quickest way to see it in action!
    </aside>

2.  **Upload Your Own Data:** If you have your own `prompts.csv`, `baseline_outputs.csv`, and optionally `finetuned_outputs.csv` files, use the file uploaders provided. You must upload at least the "Prompts CSV" and "Baseline Outputs CSV". The "Fine-Tuned Outputs CSV" is optional; if provided, the application will perform a comparison between your baseline and fine-tuned models.

After loading your data (either sample or uploaded), you'll see a sample of the combined evaluation data DataFrame, which matches prompts with model outputs, ready for analysis. This step ensures all data is correctly aligned before evaluation.

## 2. Configuring Evaluation Rules
Duration: 07:00

As Alex, configuring the evaluation rules is a critical step. This is where you define what constitutes a "good" or "bad" LLM response based on InnovateCorp's specific risk profile and the intended use case of the RAG application. By customizing these settings, you ensure the evaluation aligns with your quality standards and security considerations.

You'll configure rules across several key areas:

### Hallucination Proxy Settings

These settings help detect potential hallucinations or overly verbose responses.

*   **Hallucination Threshold Ratio:** This numerical value helps identify responses that are significantly longer than the original prompt. A very long response might indicate the LLM is generating excessive, potentially fabricated, or irrelevant information. The ratio is calculated as $$ R = \frac{\text{word count of LLM Output}}{\text{word count of Prompt Text}} $$. Adjust this to tune the sensitivity.
*   **Over-Specificity Keywords:** Enter comma-separated keywords (e.g., "exactly", "precisely") that, when present alongside unverified numerical details, might signal a hallucination. The LLM might sound confident but be factually incorrect.

### Refusal and Over-Compliance Settings

These rules help identify when an LLM inappropriately refuses to answer or provides excessive disclaimers, potentially hindering user experience or compliance.

*   **Refusal Phrases:** List phrases (one per line) that indicate the LLM is refusing to answer a prompt. This is important for identifying both appropriate and inappropriate refusals.
*   **Excessive Safety Disclaimers:** List phrases (one per line) that suggest the LLM is being overly cautious or providing unnecessary disclaimers, which can detract from the user experience without adding real value.

### Regression Analysis Settings

If you're comparing a fine-tuned model against a baseline, this setting is vital for detecting negative impacts.

*   **Regression Threshold Delta:** This numerical value (e.g., 0.05 for 5%) defines the acceptable increase in "negative" metrics (like hallucination rate or refusal rate) for the fine-tuned model. If a negative metric increases beyond this threshold, it will be flagged as a regression.

### Citation Pattern

For RAG applications, citing sources is crucial for faithfulness.

*   **Citation Pattern (regex):** Provide a regular expression that the application will use to identify citations within the LLM's output (e.g., `\[\d+\]` to match `[1]`, `[2]`). This helps in verifying if cited sources are actually present in the `allowed_sources`.

After making your adjustments, the "Current Evaluation Configuration" section will display a JSON representation of your chosen rules. Review this to ensure all settings are as intended.

## 3. Running the Evaluation Harness
Duration: 03:00

With your data loaded and evaluation rules precisely configured, Alex is now ready to unleash the full power of the evaluation harness. This step is where the system processes each prompt-response pair according to your defined rules and calculates performance metrics.

Click the **"Run Evaluation"** button.

<aside class="negative">
<b>Warning:</b> Ensure you have loaded data on the "Data Upload" page before attempting to run the evaluation. The button will be disabled if no data is present.
</aside>

Upon clicking, the application will:

1.  **Apply Hallucination Checks:** It will analyze LLM outputs against the defined hallucination threshold and over-specificity keywords.
2.  **Apply Faithfulness Checks:** For RAG scenarios, it will verify if the LLM's outputs align with `allowed_sources` and if any cited sources are valid according to your citation pattern.
3.  **Apply Refusal and Over-Compliance Checks:** It will identify if the LLM's response contains refusal phrases or excessive safety disclaimers.
4.  **Aggregate Metrics:** All individual checks are then consolidated into overall scores and rates (e.g., hallucination rate, faithfulness score, refusal rate) for each model.
5.  **Perform Regression Analysis (if applicable):** If you provided fine-tuned model outputs, the application will compare its performance against the baseline, using your "Regression Threshold Delta" to flag any undesirable regressions.

Once the evaluation is complete, a success message will appear, indicating that the results are ready for review in the subsequent pages. This aggregated view is crucial for stakeholders to get a high-level understanding of the model's trustworthiness.

## 4. View Scorecards
Duration: 06:00

Now that the evaluation has run, Alex needs to present the findings in a clear and concise manner for Maria (Model Validator) and David (AI Risk Lead). This step focuses on visualizing the aggregated metrics and identifying overall trends and risks.

### Aggregate Scorecard

The first thing you'll see is the **Aggregate Scorecard**, presented as a table. This provides a high-level overview of each model's performance across key metrics, such as:

*   **Hallucination Rate:** The percentage of prompts where hallucinations were detected.
*   **Faithfulness Score:** An indicator of how well the LLM's output is grounded in provided sources.
*   **Refusal Rate:** The percentage of prompts where the LLM refused to answer.
*   **Inappropriate Refusal Rate:** A specific subset of refusals that are deemed unhelpful or incorrect.

This scorecard offers an immediate, actionable summary of the LLM's trust posture.

### Comparison of Key LLM Evaluation Metrics

To provide a visual comparison, the application generates a **bar plot**. This chart visually contrasts the performance of the baseline model against the fine-tuned model (if provided) for metrics like `hallucination_rate`, `faithfulness_score`, and various `refusal_rate` types. Visualizations make it easier for stakeholders to quickly grasp performance differences and identify areas of concern or improvement.

### Fine-Tuning Regression Analysis

If you evaluated a fine-tuned model, this section is particularly important. It will display:

*   **Metric Deltas:** The change in performance for each metric between the baseline and fine-tuned models. A positive delta for negative metrics (like hallucination rate) indicates a worsening performance.
*   **Regression Flags:** If any negative metric for the fine-tuned model crosses your specified "Regression Threshold Delta," the application will prominently display a "**!!! REGRESSION DETECTED !!!**" warning, listing the metrics that have regressed.

This quantitative evidence is essential for Maria to make an informed decision on whether the fine-tuned model is fit for release or requires further iterations due to newly introduced risks. If no regressions are detected, it indicates that the fine-tuning appears stable or improved.

## 5. Inspect Failure Exemplars
Duration: 08:00

While aggregate scores provide a high-level view, Alex knows that understanding the *why* behind the numbers requires diving into specific examples of model failures. This "Failure Exemplars" section is designed to do just that, offering concrete illustrations of problematic LLM behavior. This granular view is crucial for root cause analysis and informs subsequent model iterations or prompt engineering strategies.

The application automatically identifies and displays a selection of "high-risk" prompts – those where one or more negative flags (e.g., hallucination, unfaithful, refusal) were triggered for either the baseline or fine-tuned model.

For each exemplar, you'll see:

*   **Prompt ID:** A unique identifier for the prompt.
*   **Prompt Text:** The original query posed to the LLM.
*   **Allowed Sources (if applicable):** The relevant documents or information the RAG system was allowed to use.
*   **Expected Answer (if applicable):** The desired outcome for the prompt.

Below this, you'll find a side-by-side comparison (if a fine-tuned model was provided):

### Baseline Model Output

*   The actual `llm_output` generated by the baseline model.
*   `Sources Cited`: Any sources the baseline model explicitly mentioned.
*   **Detected Baseline Issues:** A list of specific flags (e.g., "Excessive Length Flag", "Unsupported Factual Claim Flag", "Refusal Flag") that were triggered for this response. If no issues are detected, it will be clearly stated.

### Fine-Tuned Model Output (if applicable)

*   The actual `llm_output` generated by the fine-tuned model.
*   `Sources Cited`: Any sources the fine-tuned model explicitly mentioned.
*   **Detected Fine-Tuned Issues:** A list of specific flags triggered for this response.

### Comparison Note

A special note will highlight if the fine-tuned model showed a regression (i.e., introduced a new issue) or an improvement for that specific prompt compared to the baseline. For example, you might see a case where the baseline hallucinated, but the fine-tuned model corrected it (an improvement), or vice-versa (a regression).

<aside class="positive">
<b>Use Case:</b> For a prompt like `P002`, Alex might observe that the baseline hallucinated a wrong percentage and source, while the fine-tuned model provided a correct and grounded answer – a clear improvement. Conversely, for `P006`, the fine-tuned model might introduce an inappropriate refusal, confirming a regression seen in the aggregate metrics.
</aside>

This detailed breakdown helps Alex perform root cause analysis: Was the fine-tuning data flawed? Did new safety guardrails overcorrect? This directly informs subsequent model iterations and prompt engineering strategies, bridging the gap between raw data and actionable model improvements.

## 6. Exporting Evaluation Artifacts
Duration: 04:00

The final and crucial step for Alex is to generate all necessary evaluation artifacts. These artifacts serve as an auditable record for Maria (Model Validator) and David (AI Risk Lead), providing comprehensive evidence for model approval, compliance, and future risk assessments. This adheres to InnovateCorp's strict governance requirements.

Click the **"Generate & Download Artifacts"** button.

<aside class="negative">
<b>Warning:</b> Artifacts can only be generated after the evaluation has been run on the "Run Evaluation" page. The button will be disabled if no evaluation results are available.
</aside>

Upon clicking, the application will:

1.  **Generate a Unique Run ID:** A timestamp-based ID (e.g., `20231027_153045`) is created to uniquely identify this evaluation run.
2.  **Create Detailed Reports:** The system generates several files, including:
    *   An `executive_summary.md` with high-level findings.
    *   Detailed JSON files containing the `eval_df`, aggregate metrics, regression analysis results, and the exact configuration used for the evaluation.
3.  **Bundle into a ZIP file:** All generated reports are compressed into a single ZIP file (e.g., `Session_08_20231027_153045.zip`).
4.  **Create an Evidence Manifest:** A crucial file, `prompt_YOUR_RUN_ID_evidence_manifest.json`, is created. This manifest lists all generated artifacts along with their SHA-256 cryptographic hashes.

After generation, a **"Download All Artifacts"** button will appear, allowing you to save the ZIP file to your local machine.

### Evidence Manifest

The application will also display the content of the `evidence_manifest.json` directly on the page. This manifest is key to maintaining data integrity and traceability:

```json
{
  "run_id": "20231027_153045",
  "timestamp": "2023-10-27T15:30:45.123456",
  "artifacts": [
    {
      "filename": "executive_summary.md",
      "sha256": "abcdef12345..."
    },
    {
      "filename": "prompt_eval_results.json",
      "sha256": "ghijkl67890..."
    }
    // ... more artifact entries
  ]
}
```

This ensures that the generated evidence is tamper-proof and provides a clear audit trail for the LLM's trustworthiness, fulfilling InnovateCorp's strict security and governance requirements. This complete package enables informed decision-making and establishes a clear audit trail for the LLM's trustworthiness.
